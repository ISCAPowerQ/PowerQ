# üî¨ PowerQ: A Cross-Layer Power-Law-Aware Quantization Accelerator for LLM Inference

This repository contains the source code for **PowerQ**, a cross-layer quantization framework and accelerator design for efficient large language model (LLM) inference.

> üìù This repository is released as part of our **ISCA 2026 submission**.  
> All author-identifying information has been **removed or anonymized** in accordance with the double-blind review policy.

---

## üìÇ Repository Structure

- `Algorithm/`  
  Contains the implementation of PowerQ's quantization algorithms, calibration routines, and evaluation pipeline.

- `Hardware/`  
  Contains the simulator and scripts for different accelerators, including PowerQ.
---

## ‚öôÔ∏è Quick Start

### 1. Setup the Python Environment

```bash
cd Algorithm
conda create -n PowerQ python=3.10 -y
conda activate PowerQ
```

### 2. Run Algorithm Module

```bash
bash run_exp.sh
```

> ‚ö†Ô∏è Make sure to adjust the model and dataset paths in `run_exp.sh` as needed.

### 3. Run Hardware Simulation

```bash
cd ../Hardware
conda activate PowerQ  # Reuse the same environment

# Set HuggingFace cache path
export HF_HOME="your/HF_HOME/directory"

# Profile model shapes
bash run_shape_profile.sh

# Run inference simulation on different accelerator backends
python test_baseline.py --is_generation   # Baseline FP16
python test_ant.py      --is_generation   # ANT
python test_olive.py    --is_generation   # OliVe
python test_bitmod.py   --is_generation   # BitMoD
python test_powerq.py   --is_generation   # PowerQ (proposed)
```

## üì¨ Contact

Due to the double-blind review process, we are unable to disclose author information at this time.  

---

## üõ°Ô∏è License

This code is provided solely for the purpose of academic peer review for ISCA 2026.
Redistribution, commercial use, and modification are **not permitted** during the review process.
A full license will be released upon paper acceptance.
